# 基于 TensorFlow 的深度学习图像识别模型的自动化测试

### 项目介绍

####**任务目标：** 

- 针对深度学习图像识别模型的自动化测试框架，设计并实现一个 Python 实现

的基于 TensorFlow 的深度学习图像识别模型的自动化测试方法，采用特定的方

式，根据提供的训练数据集和待测数据集，由待测数据集尽量生成使得模型出错但

是和原始数据“相似度”高的测试数据。 

####**测试对象：** 

- 针对某个图像识别模型的待测数据集进行测试。对模型未知，对训练数据集和

待测数据集已知。其中训练数据集将使用 Fashion-MNIST 数据集。（黑盒测试）

### 代码结构

![image-20191121223937179](C:\Users\SKY\AppData\Roaming\Typora\typora-user-images\image-20191121223937179.png)

- attack_data (dir)、test_data (dir)、generate.py (file)、model.h5(file) 在根目录下

### 如何运行

- 运行**main.py**: 
  - **注意**： **tensorflow 版本**为**2.0.0** ，**python** 版本为 **3.7**
  - 根据test_data**生成**并且**储存**10000个对抗样本，打印一些运行参数
    - 打印下文“**生成对抗样本时间**”章节中提到的东西
  - 调节main函数中的count可以改变输入的样本数量（前count个）

- 运行**generate.py**：
  - 根据test_data**生成**100个对抗样本，用于**测试**
  - 调节main函数中的count可以改变输入的样本数量（前count个）
  - main函数里面有很多被注释掉的代码，取消注释可以运行（不一定能运行成功），执行一些额外功能，比如内存监控，模型评估等等。
- 单独调用 generate.py中的**generate函数**：
  - 输入为“归一化”之后的图片数据
- 运行**getModel.py**： 
  - 生成模型( model.h5 )
- 运行**show.py**：
  - **注意**： 需要额外引入 **matplotlib** 模块
  - 生成10张样本图片攻击前后的对比
  - 调节show(index)函数的index参数，可以显示从index开始的十张图片
- **model.h5** 以及 **model文件夹**：
  - model.h5
    -  通过 model = keras.models.load_model("model.h5") 的方式载入
    - 使用 model.predict 进行预测，model.evaluate来评估模型
    - 使用此模型有内存泄漏风险
  - model文件夹
    - 通过 model = tf.keras.experimental.load_from_saved_model("model") 载入
    - 使用 model.predict 进行预测，不能评估模型
    - 使用该模型预测速度极快，且不存在内存泄漏风险

### 生成对抗样本时间

- 总共用时 7分50秒

- generate.py 中 main 函数打印结果：
  - No.10000      Success: 8776/10000      Success_rate: 0.88      Runtime: 470.42s      sim: 0.78
    - No. 表示第10000用例
    - Sucess 表示攻击成功个数
    - Success_rate 表示攻击成功比例
    - Runtime表示运行时间
      - 10000用例总共耗时470.42s ， 约7分50秒
    - sim： 表示第No.n个样本的结构相似性

### 算法详解

- 先是受到单像素点攻击的启发，决定随机攻击图片上面的像素点
- 以文献《[ Simple Black-box Adversarial Attacks ](http://proceedings.mlr.press/v97/guo19a/guo19a.pdf)》描述的算法作为基本思想

![image-20191121210724250](C:\Users\SKY\AppData\Roaming\Typora\typora-user-images\image-20191121210724250.png)

- 其中，x表示输入的图片，y表示预期输出，我的算法中使用了预期的标签作为y，epsilon表示攻击程度,另外在我的算法中忽略了Q变量。
- 算法中，p表示预测成功的概率，即生成预期标签的概率，这里需要使用到训练出的模型来计算。 ( model.predict() )。
- 随机产生攻击位置，先正向改变像素点的值(加上 epsilon)，如果p没有下降，再反向改变像素的值( 减去 epsilon )。如果攻击成功，将攻击的位置标记为攻击成功，并且将p赋值为p'，p'表示降低后的预测值。如果两次攻击均失败，即p没有下降，则将位置标记为攻击无效，之后不再攻击这些点。
- 当攻击成功次数达到一定值(代码中默认为 5 )、或者尝试攻击次数达到一定值（默认为100，为了防止死循环，即一直攻击失败）、或者攻击成功（当label对应的预测值p小于某个程度之后，可以看成是攻击成功，这里选取0.3作为度量）。
- 但是最终发现单像素点攻击的效果不好，于是想到将攻击成功的像素延展开来，以某种规则攻击附近像素。经过研究发现，以该像素为中心的十字形攻击方式效果出乎意外的好，于是采用此法进行攻击。
- 采用十字形法攻击时，如果某次攻击不成功，则将十字所覆盖的范围全部标记为攻击无效，以后不再攻击。
- 最终，用该算法生成的对抗样本来评估原模型，发现原模型针对对抗样本准确率下降到了16%左右，另外对抗样本与原模型的平均结构相似度(ssim)是85%左右，说明攻击效果不错。

### 个人感受

- 一开始其实是最难的，因为对机器学习基本上一无所知。甚至不知道搜索的关键词是什么。后来将范围逐渐缩小，大概明白大作业分为两个步骤，首先是训练模型，然后是通过利用模型生成对抗样本。
- 训练模型其实没怎么花时间，很多博客上写的比较清楚； 但是黑盒攻击相关的信息就非常少了。甚至很多链接，标题写的是黑盒测试，但是实际内容是白盒测试。
- 最后在国内的博客上找到了一种方法，就是像素点攻击方法。然后又去查国内外的文献，发现了上文所述的简单有效的黑盒攻击方式，于是开始着手编写黑盒攻击相关代码。
- 开始是基于像素点攻击的，跑出来的结果并不理想，无论怎么调参，攻击成功率都十分低，或者说该方法只针对某些图片效果比较好，但是对另一些图片无能为力。
- 在一番资料查阅以及和同学讨论之后，发现了一种成功率比较高的攻击方式，就是上文所述的十字形攻击方法。在使用该方法以后，攻击成功率大大提升。然后又花了一些心思调参，最终代码定型。
- 在跑比较少的样本的时候代码显得没问题，但是，当样本大于800左右的时候，控制台总会异常地崩溃。后来追根朔源，发现是内存泄漏引起的。于是查阅了相关资料，通过一些测试方法发现是tensorflow导致地内存泄漏。之后又查阅了无数资料都无法解决内存溢出的问题。于是只能减缓代码运行速度，让代码有充分的时间进入虚拟内存，使进程不至于被操作系统突然杀死。 
- 当我以为就这么结束了的时候，某同学突然找到了一种解决办法，就是将模型以某种不被推荐的方式存入文件夹之中，这样读取出来的模型就不会存在内存泄漏问题。当然，更为奇妙的是，以这种方式读取出来的模型大大提高了代码运行速率（ 之前每秒能处理3张图，处理完10000张得用1个小时；之后每秒能处理21张图，速度提升了7倍）。
- 总的来说，这次作业不但让我了解了机器学习的知识，更重要的是，锻炼了我从零开始做好一件事情的能力，以及解决各种问题的能力。

### 参考文献和网址

- 《[ Simple Black-box Adversarial Attacks ](http://proceedings.mlr.press/v97/guo19a/guo19a.pdf)》

- [利用TensorFlow进行Fashion MNIST数据集的基本分类问题](https://blog.csdn.net/m0_37393514/article/details/81010587)

- [One pixel 对抗攻击_学习笔记](https://blog.csdn.net/qq_35414569/article/details/82148305)